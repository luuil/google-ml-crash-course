## 降低损失 (Reducing Loss)：梯度下降法

预计用时：10 分钟

[迭代方法图][p-iterative]包含一个标题为“计算参数更新”的华而不实的绿框。现在，我们将用更实质的方法代替这种华而不实的算法。

假设我们有时间和计算资源来计算 `w1` 的所有可能值的损失。对于我们一直在研究的回归问题，所产生的损失与 `w1` 的图形始终是凸形。换言之，图形始终是碗状图，如下所示：

![loss1][p-loss-1]

图 1. 回归问题产生的损失与权重图为凸形。

**凸形问题只有一个最低点**；即只存在一个斜率正好为 `0` 的位置。这个最小值就是损失函数收敛之处。

通过计算整个数据集中 `w1` 每个可能值的损失函数来找到收敛点这种方法效率太低。我们来研究一种更好的机制，这种机制在机器学习领域非常热门，称为**梯度下降法**。

梯度下降法的第一个阶段是为 `w1` 选择一个起始值（**起点**）。起点并不重要；因此很多算法就直接将 `w1` 设为 `0` 或随机选择一个值。下图显示的是我们选择了一个稍大于 `0` 的起点：

![loss2][p-loss-2]

图 2. 梯度下降法的起点。

然后，梯度下降法算法会计算损失曲线在起点处的梯度。简而言之，**梯度**是偏导数的矢量；它可以让您了解哪个方向距离目标“更近”或“更远”。请注意，损失相对于单个权重的梯度（如图 2 所示）就等于导数。

> [详细了解偏导数和梯度](#详细了解偏导数和梯度)

请注意，梯度是一个矢量，因此具有以下两个特征：

- 方向
- 大小

梯度始终指向损失函数中增长最为迅猛的方向。梯度下降法算法会沿着负梯度的方向走一步，以便尽快降低损失。

![loss3][p-loss-3]

图 3. 梯度下降法依赖于负梯度。

为了确定损失函数曲线上的下一个点，梯度下降法算法会将梯度大小的一部分与起点相加，如下图所示：

![loss4][p-loss-4]

图 4. 一个梯度步长将我们移动到损失曲线上的下一个点。

然后，梯度下降法会重复此过程，逐渐接近最低点。


### 详细了解偏导数和梯度

#### 偏导数

多变量函数指的是具有多个参数的函数，例如：

```
f(x,y)=e^(2y)*sin(x)
```

`f` 相对于 `x` 的偏导数表示如下：

```
∂f/∂x
```

是 `f (x)` 的导数。要计算以下值：

```
∂f/∂x
```

您必须使 `y` 保持固定不变（因此 `f` 现在是只有一个变量 `x` 的函数），然后取 `f` 相对于 `x` 的常规导数。例如，当 `y` 固定为 `1` 时，前面的函数变为：

```
f(x)=e^2*sin(x)
```

这只是一个变量 `x` 的函数，其导数为：

```
e^2*cos(x)
```

一般来说，假设 `y` 保持不变，`f` 对 `x` 的偏导数的计算公式如下：

```
∂f/∂x*(x,y)=e^(2y)*cos(x)
```

同样，如果我们使 `x` 保持不变，`f` 对 `y` 的偏导数为：

```
∂f/∂y*(x,y)=2e^(2y)*sin(x)
```

直观而言，偏导数可以让您了解到，当您略微改动一个变量时，函数会发生多大的变化。在前面的示例中：

```
∂f/∂x*(0,1)=e^2≈7.4
```

因此，如果您将起点设为 `(0,1)`，使 `y` 保持固定不变并将 `x` 移动一点，`f` 的变化量将是 `x` 变化量的 `7.4` 倍左右。

在机器学习中，偏导数主要与函数的梯度一起使用。

#### 梯度

函数的梯度是偏导数相对于所有自变量的矢量，表示如下：

```
∇f
```

例如，如果：

```
f(x,y)=e^(2y)*sin(x)
```

则：

```
∇f(x,y)=(∂f/∂x*(x,y), ∂f/∂y*(x,y))=(e^(2y)*cos(x), 2e^(2y)*sin(x))
```

请注意以下几点：

- `∇f`: 指向函数增长速度最快的方向。
- `−∇f`: 指向函数下降速度最快的方向。

该矢量中的**维度**个数等于 `f` 公式中的变量个数；
换言之，该矢量位于该函数的域空间内。例如，在三维空间中查看下面的函数 `f(x,y)` 时：

```
f(x,y)=4+(x−2)2+2y2
```

`z = f(x,y)` 就像一个山谷，最低点为 `(2,0,4)`：

![f(x,y)][p-fxy]

`f(x,y)` 的梯度是一个二维矢量，可让您了解向哪个 `(x,y)` 方向移动时高度下降得最快。也就是说，梯度矢量指向山谷。

在机器学习中，梯度用于梯度下降法。我们的损失函数通常具有很多变量，而我们尝试通过跟随函数梯度的负方向来尽量降低损失函数。

[p-iterative]: ../image/03-A-iterative-1.png
[p-loss-1]: ../image/03-B-loss-1.png
[p-loss-2]: ../image/03-B-loss-2.png
[p-loss-3]: ../image/03-B-loss-3.png
[p-loss-4]: ../image/03-B-loss-4.png
[p-fxy]: ../image/03-B-fxy-1.png