## 降低损失 (Reducing Loss)

为了训练模型，我们需要一种可降低模型损失的好方法。迭代方法是一种广泛用于降低损失的方法，而且使用起来简单有效。

### 学习目标

- 了解如何使用迭代方法来训练模型。
- 全面了解梯度下降法和一些变体，包括：
    - 小批量梯度下降法(mini-batch Gradient Descent)
    - 随机梯度下降法(Stochastic Gradient Descent)
- 尝试不同的学习速率。

### 内容

#### 如何降低损失？

- `(y - y')2` 相对于权重和偏差的导数可让我们了解指定样本的损失变化情况
- 易于计算且为凸形
- 因此，我们在能够尽可能降低损失的方向上反复采取小步
    - 我们将这些小步称为**梯度步长**（但它们实际上是负梯度步长）
    - 这种优化策略称为**梯度下降法**

#### 权重初始化

- 对于凸形问题，权重可以从任何位置开始（比如，所有值为 0 的位置）
    - 凸形：想象一个碗的形状
    - 只有一个最低点
- Foreshadowing：不适用于神经网络
    - 非凸形：想象一个蛋托的形状
    - 有多个最低点
    - 很大程度上取决于初始值

#### SGD 和小批量梯度下降法

- 可以在每步上计算整个数据集的梯度，但事实证明没有必要这样做
- 计算小型数据样本的梯度效果很好
    - 每一步抽取一个新的随机样本
- **随机梯度下降法**：一次抽取一个样本
- **小批量梯度下降法**：每批包含 10-1000 个样本
    - 损失和梯度在整批范围内达到平衡

### 更多

- [An iterative approach](A-an-iterative-approach.md)
- [Gradient descent](B-gradient-descent.md)
- [Learning rate](C-learning-rate.md)
- [Stochastic gradient descent](D-stochastic-gradient-descent.md)